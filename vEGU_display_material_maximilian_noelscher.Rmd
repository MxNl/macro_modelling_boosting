---
title: "Using an Extreme Gradient Boosting Learner for Mapping Hydrogeochemical Parameters in Germany"
subtitle: "Display Material"
author: "Maximilian NÃ¶lscher, Stefan Broda"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  bookdown::html_document2:
    theme: lumen
    highlight: tango
    toc: TRUE
    toc_float: TRUE
    collapse: FALSE
    toc_depth: 3
    collapsed: FALSE
    smooth_scroll: FALSE
    number_sections: TRUE
    self_contained: yes
    df_print: paged
    #code_folding: show
bibliography: vEGU_display_material.bib
nocite: '@*'
biblio-style: "apalike"
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(fig.align="center")
options(tidyverse.quiet = TRUE)
```

```{r}
library(skimr)
library(visdat)
library(targets)
library(mapview)
library(knitr)
library(sf)
library(ggExtra)
library(patchwork)
library(tidyverse)
```

```{r, echo = FALSE}
source("R/processing_functions.R")
source("R/markdown.R")
source("R/plot_functions.R")
source("R/constants.R")
```

<br />
This study is part of the [MACRO project](https://www.bgr.bund.de/EN/Themen/Wasser/Projekte/laufend/F+E/Macro/macro_projektbeschr_en.html?nn=1548270). This project will end approximately in the summer of 2023. Briefly, its goals are to explore the application of machine learning for mapping/regionalization in the field of hydrogeology in general.
<br />
<br />

```{r, fig.cap="MACRO project logo"}
knitr::include_graphics("logo/MACRO_logo.svg")
```
<br />
<br />
<br />
<br />
<br />

# Abstract

Information on the spatial distribution of hydrogeochemical parameters is crucial for decision making. Machine learning based methods for the mapping of hydrogeochemical parameter concentrations have been already studied for many years to evolve from deterministic and geostatistical interpolation methods. However, the reflection of all relevant processes that the target variables depend on is often difficult to achieve, because of the mostly insufficient determination and/or availability of features. This is especially true if you limit yourself to freely accessible data.

In this study, we apply an extreme gradient boosting learner (XGBoost) to map major ion concentrations across Germany. The training data consists of water samples from approximately 35K observation wells across Germany and a wide range of environmental data as predictors. The water samples were collected between the 1950s and 2005 at anthropogenically undisturbed locations.

The environmental data includes hydrogeological units and parameters, soil type, lithology, digital elevation model (DEM) and DEM derived parameters etc. The values of these features at the respective water sample location were extracted on the basis of a polygon, approximately representing the area that has an impact on the target variable (ion concentration). For a comparison, different polygon shapes are used.

# Workflow

The workflow from data preprocessing to model evaluation is schematically shown in fig. \@ref(fig:flowchart). The single steps are described more in detail in the following sections. This work of this study is still in progress. The preprocessing of the hydrogeochemical background values data set is described in [Hydrogeochemical Parameters], the feature extraction in section [Feature Extraction], the modelling pipeline in [Model Training] and the evaluation can be found in [Evaluation].

All data processing and modelling was done using the R programming language using multiple packages (see chapter [References]).

```{r flowchart, fig.cap="flowchart", out.width="50%"}
knitr::include_graphics("inkscape_figures/flowchart.svg")
```

# Data

## Hydrogeochemical Parameters
```{r}
original_data <- tar_read(back_vals_2005_clean_sf)
```

The target variables of the training data being used in this study is based on a data set containing initially approximately `r as.character(round(nrow(original_data), -3))` measurements of hydrogeochemical parameters from groundwater samples predominantly taken during the second half of the 20th century until `r original_data %>% pull(date) %>% max(na.rm = TRUE) %>% lubridate::year()`. The number of samples used for training the models is reduced due to the following steps:

### Preprocessing

<br />
**Intersection with Study Area**
<br />

The sample locations were intersected with the administrative border of Germany.

<br />
**Filter by Sample Date**
<br />

Only samples between `r MIN_SAMPLE_DATE` and `r original_data %>% pull(date) %>% max(na.rm = TRUE)` were kept to limit the data set to the most current time period. The distribution of the sample date after applying the previous processing steps is shown in fig. \@ref(fig:sampledatedistribution).

```{r sampledatedistribution, fig.cap="Distribution of sample date"}
original_data %>%
  as_tibble() %>%
  select(date) %>%
  mutate(
    date_cat = if_else(date >= MIN_SAMPLE_DATE, "kept", "discarded"),
    date_cat = factor(date_cat, levels = c("discarded", "kept"))
  ) %>%
  ggplot(aes(date, fill = date_cat)) +
  geom_histogram(
    alpha = .4,
    boundary = as.Date(MIN_SAMPLE_DATE)
  ) +
  geom_vline(
    xintercept = as.Date(MIN_SAMPLE_DATE),
    colour = "grey",
    linetype = "dashed"
  ) +
  geom_label(
    data = data.frame(x = as.Date(c("1975-11-27", "1975-11-27")), y = c(18598.5577000015, 18598.5577000015), label = c(
      MIN_SAMPLE_DATE,
      MIN_SAMPLE_DATE
    )), mapping = aes(x = x, y = y, label = label),
    label.padding = unit(0.25, "lines"), label.r = unit(
      0.1,
      "lines"
    ), inherit.aes = FALSE,
    colour = "grey"
  ) +
  geom_curve(data = data.frame(
    x = as.Date("1981-11-17"), y = 17123.0310652555,
    xend = as.Date("1988-04-02"), yend = 15336.8672442472
  ), mapping = aes(
    x = x,
    y = y, xend = xend, yend = yend
  ), arrow = arrow(30L, unit(
    0.1,
    "inches"
  ), "last", "closed"), inherit.aes = FALSE, colour = "grey") +
  theme_minimal() +
  theme(legend.position = "top", legend.direction = "horizontal") +
  labs(
    fill = "Filter on sample date",
    x = "Sample date",
    y = "Number of samples"
  )
```
<br />
**Filter by Sample Depth**
<br />

The sample depth was calculated as mean depth of the screen top and bottom if it was provided.
Thus, it reflects the screen center depth below ground level of an observation well. For the model training, the sample depth was used as feature (predictor variable).

Only samples with a sample depth between `r MAX_SAMPLE_DEPTH`m and ground level or with a value of 1 in the column `lage` were kept in the data set to exclude deeper aquifers. The distribution of the sample depth after applying the previous processing steps is shown in figure \@ref(fig:sampledepthdistribution).

```{r sampledepthdistribution, fig.cap="Distribution of sample depth"}
original_data %>%
  as_tibble() %>%
  filter(date >= MIN_SAMPLE_DATE) %>%
  select(sample_depth) %>%
  mutate(
    depth_cat = if_else(sample_depth <= MAX_SAMPLE_DEPTH, "kept", "discarded"),
    depth_cat = factor(depth_cat, levels = c("discarded", "kept"))
  ) %>%
  ggplot(aes(sample_depth, fill = depth_cat)) +
  geom_histogram(
    alpha = .4,
    boundary = MAX_SAMPLE_DEPTH
  ) +
  scale_x_continuous(
                 limits = c(-10, 500)) +
  geom_vline(
    xintercept = MAX_SAMPLE_DEPTH ,
    colour = "grey",
    linetype = "dashed"
  ) +
  geom_label(
    data = data.frame(x = c(200, 200), y = c(8000, 8000), label = c(
      str_c(MAX_SAMPLE_DEPTH, " m below ground level")
    )), mapping = aes(x = x, y = y, label = label),
    label.padding = unit(0.25, "lines"), label.r = unit(
      0.1,
      "lines"
    ), inherit.aes = FALSE,
    colour = "grey"
  ) +
  geom_curve(data = data.frame(
    x = 200, y = 7000,
    xend = 120, yend = 5000
  ), mapping = aes(
    x = x,
    y = y, xend = xend, yend = yend
  ), curvature = -0.305, arrow = arrow(30L, unit(
    0.1,
    "inches"
  ), "last", "closed"), inherit.aes = FALSE, colour = "grey") +
  theme_minimal() +
  theme(legend.position = "top", legend.direction = "horizontal") +
  labs(
    fill = "Filter on sample depth",
    x = "Sample depth",
    y = "Number of samples"
  )
```
<br />
**Aggregation of multiple measurements per sample site**
<br />

Some of the sample sites have multiple measurements over time which were aggregated by calculating the mean. The distribution of the number of measurements per sample site after applying the previous processing steps is shown in figure \@ref(fig:measurementdistribution).

```{r measurementdistribution, fig.cap="Distribution of multiple measurements per sample site"}
original_data %>% 
  filter(date >= MIN_SAMPLE_DATE) %>% 
  filter(sample_depth <= MAX_SAMPLE_DEPTH | lage == 1) %>% 
  as_tibble() %>% 
  # slice(40000:45000) %>% 
  group_by(station_id) %>% 
  summarise(n = n()) %>% 
  mutate(
    n = factor(n, levels = min(n):max(n)),
    n_cat = if_else(as.character(n) == as.character(1), "unchanged", "aggregated")
    # n_cat = factor(n_cat, levels = c("aggregated", "unchanged"))
  ) %>%
  ggplot(aes(n)) +
  geom_histogram(
    aes(fill = n_cat),
    alpha = .4,
    stat = "count"
  ) +
  scale_y_log10() +
  theme_minimal() +
  theme(legend.position = "top", legend.direction = "horizontal") +
  labs(
    fill = "Treatment of multiple measurements per sample site",
    x = "Number of measurements per sample site",
    y = "Number of sample sites"
  )
```

```{r}
target_data <-
  tar_read(back_vals_filter_allpositive_sf) %>%
  as_tibble() %>%
  select(station_id, sample_depth, all_of(tar_read(parameters_to_model))) %>%
  filter_all(any_vars(complete.cases(.)))

tar_read(parameters_to_model) %>% parameter_pretty_markdown() %>% word(1)
```

From all measured parameters, the ten ions with the most samples were selected as target variables to be modeled (`r tar_read(parameters_to_model) %>% parameter_pretty_markdown() %>% word(1) %>% str_c(collapse = ", ")`; see fig. \@ref(fig:samplesparameters)). Across all these parameters and after all preprocessing steps, `r nrow(target_data)` samples and `r ncol(target_data)` columns, 1 for the station ID (`station_id`), 1 for the sample depth (`sample_depth`) and `r ncol(target_data) - 2` for each target variable remain for the model training.

```{r samplesparameters, fig.height=8, fig.cap="Samples per hydrogeochemical."}
tar_read(back_vals_filter_allpositive_sf) %>%
  as_tibble() %>% 
  select(-station_id, -sample_depth, -geometry) %>% 
  pivot_longer(cols = everything()) %>% 
  drop_na(everything()) %>% 
  count(name) %>% 
  arrange(-n) %>% 
  mutate(n_cat = if_else(row_number() <= 13 & !(name %in% c("ph", "lf_u_scm", "nh4_mg_l")), "kept", "discarded")) %>% 
  ggplot(aes(reorder(name, n), n, fill = n_cat)) +
  geom_col(alpha = .4) +
  theme_minimal() +
  theme(legend.position = "top", legend.direction = "horizontal") +
  labs(
    fill = "",
    x = "Hydrogeochemical parameters",
    y = "Number of samples"
  ) +
  coord_flip()
```

### Location

The locations of the sample sites used for modelling are shown in \@ref(fig:locations) as the number of sample sites per hexagon. The spatial distribution of sampling locations is unbalanced with regions that have few locations and regions with a high density. The latter are mainly concentrated around larger cities in Germany such as Berlin, Hamburg, Frankfurt. The eastern and northern areas of Germany also generally have more sampling sites compared to southern or central Germany.

```{r locations, fig.cap="Sample site locations"}

plot <- tar_read(back_vals_filter_allpositive_sf) %>% 
  st_coordinates() %>% 
  as_tibble() %>% 
  ggplot(aes(X, Y)) +
  ggplot2::stat_bin_hex(binwidth = 1E4) +
  geom_point(fill = NA, colour = NA) +
  scale_fill_viridis_c() +
  coord_equal() +
  theme_minimal() +
  theme(legend.position = "left") +
  labs(fill = "Number of sample\nsites per hexagon")

ggExtra::ggMarginal(plot, type="density", fill = "grey", colour = NA, alpha = .3)

# mapviewOptions(maxpoints = 40000)
# 
# make_locations_overview_plot(
#   tar_read(data_features_target),
#   tar_read(back_vals_filter_allpositive_sf),
#   tar_read(parameters_to_model)
# )
```


### Data Summary
The first three rows of the data set containing the target variables after all preprocessing steps is shown in table \@ref(tab:targetdatahead) as an example.

```{r targetdatahead}
target_data %>% 
  drop_na(everything()) %>% 
  select(-sample_depth) %>% 
  slice(1:3) %>% 
  knitr::kable(caption = "First three rows of the data set containing the target variables")
```

Figure \@ref(fig:missingvalues) gives an overview on the occurrence of missing values in that data set. The occurrence of missing values varies between the different target variables which leads to different sample sizes when modelling each target separately

```{r, missingvalues, fig.cap="Missing values across the target variables"}
target_data %>%
  select(all_of(tar_read(parameters_to_model))) %>% 
  set_names(parameter_pretty_markdown(names(.), with_unit = FALSE, markdown = FALSE)) %>% 
  vis_dat(warn_large_data = FALSE)
```

More details on the column statistics are shown in the following summary

```{r}
target_data %>% 
  select(all_of(tar_read(parameters_to_model))) %>% 
  skim()
```

The distribution of target values as violin chart is shown in figure \@ref(fig:violindistribution).

```{r violindistribution, fig.cap="Distribution of the target variable values"}
tar_read(plot_xgb_violin_distribution_targets)
```

## Features

### Feature Extraction
In addition to this dataset, geophysical attributes were extracted from other spatial data sources (see the following list) and used as features:

 - [Geological Map](https://www.bgr.bund.de/DE/Themen/Sammlungen-Grundlagen/GG_geol_Info/Karten/Deutschland/GUEK200/guek200_inhalt.html) [@bgr__sgd_guk250_2021]
 - [Hydrogeological Map](https://www.bgr.bund.de/DE/Themen/Wasser/Projekte/laufend/Beratung/Huek200/huek200_projektbeschr.html) [@bgr__sgd_huk250_2019]
 - [Seepage Rate](https://www.bgr.bund.de/DE/Themen/Boden/Produkte/Karten/Downloads/Berechnung_Sickerwasserrate.pdf?__blob=publicationFile&v=4) [@bgr_swr1000_2003]
 - [Groundwater Recharge](https://www.bgr.bund.de/DE/Themen/Wasser/Projekte/abgeschlossen/Beratung/Had/Was_had_abb_gwn1000.html?nn=1546102) [@bgr_gwn1000_2019]
 - [Hydrogeological Units](https://www.bgr.bund.de/DE/Themen/Wasser/Projekte/abgeschlossen/Beratung/Hyraum/hyraum_projektbeschr.html) 
 - [Longterm Mean Temperature](https://opendata.dwd.de/climate_environment/CDC/grids_germany/multi_annual/air_temperature_mean/BESCHREIBUNG_gridsgermany_multi_annual_air_temperature_mean_7100_de.pdf) [@noauthor_vieljahrige_nodate]
 - [Longterm Mean Precipitation](https://opendata.dwd.de/climate_environment/CDC/grids_germany/multi_annual/precipitation/BESCHREIBUNG_gridsgermany_multi_annual_precipitation_6190_de.pdf)
 - Multiorder Hydrologic Position (generated for this study after @belitz_multiorder_2019, not yet published) [@noauthor_raster_nodate]
 - [Soil units](https://www.bgr.bund.de/DE/Themen/Boden/Bilder/Bod_BGL5000_g.html) [@bgr_bgl5000_2014]
 - [Land Use and Land Cover](https://land.copernicus.eu/pan-european/corine-land-cover) [@_european_union_corine_2018]
 - [Elevation](https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1.1) [@_european_union_eu-dem_nodate]
 - [Slope](https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1-0-and-derived-products/slope) [@_european_union_slope_nodate]
 - [Aspect](https://land.copernicus.eu/imagery-in-situ/eu-dem/eu-dem-v1-0-and-derived-products/aspect) [@_european_union_aspect_nodate]

The features were extracted for a 1km buffer as approximated groundwater contributing area for every sample location respectively [@knoll_large_2019] (see figure \@ref(fig:interactiveschematiclulcfeatureextractionmap)). For categorical data, the proportion of each class in the buffer was calculated. As an advantage, this leads to an encoding as numerical feature. On the other hand, many sparse features are created for rare classes. For numerical data, the mean was calculated for this buffer.

```{r interactiveschematiclulcfeatureextractionmap, fig.cap="Example of feature extraction based on circular buffer around sample sites (red) (e.g. the land use and land cover data as shown here)"}
knitr::include_graphics("plots_from_mohp_data_preparation/interactive_schematic_lulc_feature_extraction_map.svg")
```

### Data Summary

```{r}
data_features <- 
  tar_read(data_features_depth_added)
```

 The previously described method of extracting the features results in `r data_features %>% ncol() %>% magrittr::subtract(1)` features. A summary of the statistics of the features is provided in tab. \@ref(tab:statssummaryfeatures).

```{r statssummaryfeatures}
data_features %>% 
  select(-station_id) %>% 
  skim()
```

The first three rows of the data set containing the features is shown in table \@ref(tab:featuresdatahead) as an example. This table was then joined with the table holding the target variable by the `station_id`

```{r featuresdatahead}
data_features %>% 
  drop_na(everything()) %>% 
  slice(1:3) %>% 
  knitr::kable(caption = "First three rows of the data set containing the features") %>% 
  kableExtra::scroll_box(width = "800px")
```

# Model Training

Like all the data preprocessing all modelling was done using the R programming language and the `tidymodels` package.
The modelling pipeline includes the following steps:

- Normalization (Standardization) of all features 
- Removal of highly correlated and/or sparse features
- 80/20 train-test split
- 5-fold cross-validation and stratification of the target variable (no spatial cross-validation yet implemented)
- Hyperparameter tuning of the model parameters 
  * `min_n`: minimum sum of instance weight (hessian) needed in a child [@chen_xgboost_2020]
  * `tree_depth`: maximum depth of a tree [@chen_xgboost_2020]
  * `learn_rate`: learning rate [@chen_xgboost_2020]
  * `loss_reduction`: minimum loss reduction required to make a further partition on a leaf [@chen_xgboost_2020] node of the tree 

  for a XGBoost learner (number of trees fixed to 1000) with 50 parameter combinations using a parameter space filling grid


# Results
## Hyperparameter Tuning {.tabset}

The following table summaries the hyperparameters of the best model configuration according to the tuning process.
```{r besthyperparamssummary, echo = FALSE, results = "asis"}
model_params <- tar_read(xgboost_model_final_params)

model_params %>% 
  set_names(tar_read(parameters_to_model)) %>% 
  imap(~mutate(.x, parameter = .y, .before = 1)) %>% 
  reduce(bind_rows) %>% 
  mutate(parameter = parameter_pretty_rmarkdown(parameter, with_unit = FALSE)) %>% 
  kable() %>% 
  kableExtra::kable_styling(full_width = TRUE)
```


## Evaluation


*Disclaimer: As these results are preliminary and this study is still work in progress, the plain performance metrics are provided without any further statements or interpretations.*

### Performance {.tabset}
```{r, echo = FALSE, results = "asis"}
plot_observed_vs_predicted <- tar_read(plot_xgb_observed_vs_predicted)
plot_residuals_vs_predicted <- tar_read(plot_xgb_residuals_vs_predicted)

names_pretty <-
  plot_observed_vs_predicted %>%
  names() %>%
  parameter_pretty_markdown(with_unit = FALSE)

for (i in seq_along(names_pretty)) {
  create_section_results(plot_observed_vs_predicted[[i]],
                         plot_residuals_vs_predicted[[i]],
                         names_pretty[[i]],
                         4)
}

# plot_observed_vs_predicted %>%
#   names() %>%
#   parameter_pretty_markdown(with_unit = FALSE) %>%
#   list(plot_observed_vs_predicted,
#        plot_residuals_vs_predicted) %>%
#   pmap(~create_section_results(..2, ..1, ..3, 3))
```

### Feature Importance {.tabset}
```{r, echo = FALSE, results = "asis"}
plot_feature_importance <- tar_read(plot_xgb_feature_importance)

plot_feature_importance %>%
  names() %>%
  parameter_pretty_markdown(with_unit = FALSE) %>% 
  map2(plot_feature_importance, ~create_section(.y, .x, 4))
```

# Conclusion and Outlook

This study aims on setting up a benchmark model as a basis for further development of more sophisticated machine learning models. The relatively simple model approach shows that the model performance varies strongly between the target variables indicating that further features are required to reflect the geophysical characteristics which play a role in the processes that drive the concentration of the target variable. Further steps to increase this preliminary model setup are:

 - Inclusion of more and relevant geophysical attributes as features
 - Evaluation of different feature extraction/engineering methods
 - Set up of a model ensemble to approximate a XGBoost multi-output model and model all parameters in a single model
 - Implement spatial cross-validation for more realistic model performance estimation
 - Evaluate deep learning approaches
 
# References